{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tworzenie zasobów\n",
    "\n",
    "Algorytmy wykorzystywane w problemach przetwarzania języka naturalnego opierają najczęściej swoje działanie o analizę dużych korpusów danych. O ile w zadaniach konkursowych często odpowiednie dane są już przygotowane, o tyle tworząc własne eksperymenty, często musimy sami pozyskać dane i przetransformować do użytecznej postaci.\n",
    "\n",
    "Dzisiejsze laboratoria dotyczyć będą tworzenia korpusów danych.\n",
    "\n",
    "## Automatyczne pozyskiwanie surowych danych tekstowych\n",
    "Dotychczas omawiane metody działały na surowym tekście, który transformowany był do odpowiedniej reprezentacji wektorowej (Bag of words, bag of ngrams, embeddingi). Jak zautomatyzować pozyskiwanie takich surowych danych z internetu?\n",
    "\n",
    "W tej części skupimy się na stworzeniu automatycznego pobieracza danych, który działać będzie w dwóch \"obszarach\":\n",
    "<ol>\n",
    "<li>crawler: moduł odwiedzający kolejne strony internetowy</li>\n",
    "<li>scraper: moduł ekstrahujący treść z konkretnych stron internetowych</li>\n",
    "</ol>\n",
    "\n",
    "Wykorzystajmy do tego dwie biblioteki: \n",
    "\n",
    "**urllib** - do odwiedzania stron\n",
    "\n",
    "**BeautifulSoup** - do parsowania danych (np. w formacie HTML).\n",
    "\n",
    "## Zadanie1: Napisz prosty ekstraktor danych ze stron WWW odwiedzający kilka podstron\n",
    "Ekstraktor ma odwiedzić zadaną stronę internetową, pobrać zawartość wszystkich tekstów wewnątrz paragrafów (wewnątrz tagów P zawartych w pobranym dokumencie HTML), a następnie odwiedzić 5 dowolnych linków z tej strony i z nich analogicznie pobrać zawartość.\n",
    "Łącznie powinniśmy otrzymać dane z 6 adresów internetowch (strona główna + 5 linków ze strony głównej).\n",
    "\n",
    "Do napisania crawlera przydać się mogą następujące funkcje:\n",
    "\n",
    "urllib.request.urlopen() - do pobrania zawartości strony\n",
    "findAll() na obiekcie BeautifulSoup, można ją wykorzystać do przeiterowania po wszystkich tagach danego rodzaju\n",
    "get_text() - Istnieje duża szansa, że wewnątrz tagów P znajdą się również inne tagi HTML, chcielibyśmy oczyścić \n",
    "z nich tekst. Można to zrobić albo z wyrażeniami regularnymi (robiliśmy takie zadanie na pierwszych laboratoriach!), albo użyć właśnie funkcji get_text() z BeautifulSoup\n",
    "\n",
    "Linki do dokumentacji:\n",
    "urllib, pobieranie danych: https://docs.python.org/3/howto/urllib2.html\n",
    "beautifulSoup: https://www.crummy.com/software/BeautifulSoup/bs4/doc/ (przeczytanie QuickStart jest wystarczające do zrobienia tego zadania)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:00<00:00, 37605.83it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 41887.19it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 8499.10it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "100%|██████████| 11/11 [00:00<00:00, 4347.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Przetwarzanie Języka Naturalnego', 'Strona poświęcona przedmiotowi obieralnemu.', 'Przedmiot poświęcony jest zagadnieniom przetwarzania języka naturalnego.\\nSzczegółowe tematy obejmują podstawowe techniki przetwarzania tekstu (wyrażenia regularne, filtrowanie wyrazów funkcyjnych, segmentacja, lematyzacja, odległość edycyjna), klasyfikację tekstu, analizę gramatyczną, statystyczne modelowanie języka, wektorowe reprezentacje dystrybucyjne (embeddings), reprezentację wiedzy, ekstrakcję informacji i wiedzy z danych tekstowych, zagadnienia uczenia głębokiego w przetwarzaniu języka naturalnego oraz zastosowania do celów analizy sentymentu i opinii oraz w systemach dialogowych.', 'Slajdy z wykładów oraz notatniki z ćwiczeniami laboratoryjnymi znajdują się na uczelnianej platformie eLearningowej PUT LMS Moodle.\\nDla celów przedmiotu stworzone zostało również forum dyskusyjne Piazza, które można wykorzystać w celu kontaktowania się z prowadzącymi i zadawania pytań dotyczących przedmiotu. Forum znajduje się pod adresem Piazza, aby dołączyć do strony przedmiotu, należy się zarejestrować używając studenckiego adresu e-mail, poprzez kliknięcie “Students get started” -> Następnie wyszukanie “Poznan University of Technology (put.poznan.pl) i kursu PJN.', 'W semestrze odbędzie się 10 wykładów we wtorki w sali L122 BT w godzinach 9:00-11:15. Grafik wykładów możecie znaleźć poniżej.', 'Podstawowa', 'Uzupełniająca', 'Ocena na podstawie punktów, których można zdobyć łącznie 100, w następujący sposób:', 'W trakcie semestru udostępnionych zostanie 10 quizów, z terminem rozwiązania do 6 dni po wykładzie.\\nCzęść wykładu zostanie przeznaczona na prezentacje studentek i studentów, na wybrane tematy związane z przetwarzaniem języka naturalnego (terminy i forma zgłaszania tematów zostanie przekazana na wykładzie).', 'Ocena zostanie wystawiona na podstawie punktów, które można będzie zdobyć w następujący sposób.', 'Zaliczenie laboratoriów obejmuje 10 zadań do wykonania oraz projekt, który będzie można zrealizować zespołowo. Projekt zostanie oceniony na podstawie prezentacji projektu oraz głosowania przez studentki i studentów, które odbędą się na ostatnim wykładzie.', 'Skala ocen', 'Przetwarzanie Języka Naturalnego', 'Strona poświęcona przedmiotowi obieralnemu.', 'Przedmiot poświęcony jest zagadnieniom przetwarzania języka naturalnego.\\nSzczegółowe tematy obejmują podstawowe techniki przetwarzania tekstu (wyrażenia regularne, filtrowanie wyrazów funkcyjnych, segmentacja, lematyzacja, odległość edycyjna), klasyfikację tekstu, analizę gramatyczną, statystyczne modelowanie języka, wektorowe reprezentacje dystrybucyjne (embeddings), reprezentację wiedzy, ekstrakcję informacji i wiedzy z danych tekstowych, zagadnienia uczenia głębokiego w przetwarzaniu języka naturalnego oraz zastosowania do celów analizy sentymentu i opinii oraz w systemach dialogowych.', 'Slajdy z wykładów oraz notatniki z ćwiczeniami laboratoryjnymi znajdują się na uczelnianej platformie eLearningowej PUT LMS Moodle.\\nDla celów przedmiotu stworzone zostało również forum dyskusyjne Piazza, które można wykorzystać w celu kontaktowania się z prowadzącymi i zadawania pytań dotyczących przedmiotu. Forum znajduje się pod adresem Piazza, aby dołączyć do strony przedmiotu, należy się zarejestrować używając studenckiego adresu e-mail, poprzez kliknięcie “Students get started” -> Następnie wyszukanie “Poznan University of Technology (put.poznan.pl) i kursu PJN.', 'W semestrze odbędzie się 10 wykładów we wtorki w sali L122 BT w godzinach 9:00-11:15. Grafik wykładów możecie znaleźć poniżej.', 'Podstawowa', 'Uzupełniająca', 'Ocena na podstawie punktów, których można zdobyć łącznie 100, w następujący sposób:', 'W trakcie semestru udostępnionych zostanie 10 quizów, z terminem rozwiązania do 6 dni po wykładzie.\\nCzęść wykładu zostanie przeznaczona na prezentacje studentek i studentów, na wybrane tematy związane z przetwarzaniem języka naturalnego (terminy i forma zgłaszania tematów zostanie przekazana na wykładzie).', 'Ocena zostanie wystawiona na podstawie punktów, które można będzie zdobyć w następujący sposób.', 'Zaliczenie laboratoriów obejmuje 10 zadań do wykonania oraz projekt, który będzie można zrealizować zespołowo. Projekt zostanie oceniony na podstawie prezentacji projektu oraz głosowania przez studentki i studentów, które odbędą się na ostatnim wykładzie.', 'Skala ocen', 'Username or Email Address', 'Password', 'Remember Me', 'Lost your password?', '← Back to Przetwarzanie Języka Naturalnego', 'WordPress is open source software you can use to create a beautiful website, blog, or app.', 'Beautiful designs, powerful features, and the freedom to build anything you want. WordPress is both free and priceless at the same time.', '35% of the web uses WordPress, from hobby blogs to the biggest news sites online.', 'Discover more sites built with WordPress.', 'Limitless possibilities. What will you create?', 'Extend WordPress with over 55,000 plugins to help your website meet your needs. Add an online store, galleries, mailing lists, forums, analytics, and much more.', 'Hundreds of thousands of developers, content creators, and site owners gather at monthly meetups in 817 cities worldwide.', 'Over 60 million people have chosen WordPress to power the place on the web they call “home” — join the family.', 'April continued to be a challenging time for the WordPress community, with many under stay-at-home recommendations. However, it was also an exciting month in which we created new ways to connect with and inspire each other! This month, amazing contributors moved more WordCamps online and shipped new releases for WordPress and Gutenberg. For the latest, […]', '… and hundreds more', 'Code is Poetry.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "\n",
    "URL = \"http://www.cs.put.poznan.pl/alawrynowicz/pjn/\"\n",
    "\n",
    "# wziete ze stackoverflow (aby nie instalowac do tego pakietu)\n",
    "def is_url(url) -> bool:\n",
    "    import re\n",
    "    regex = re.compile(\n",
    "        r'^(?:http|ftp)s?://' # http:// or https://\n",
    "        r'(?:(?:[A-Z0-9](?:[A-Z0-9-]{0,61}[A-Z0-9])?\\.)+(?:[A-Z]{2,6}\\.?|[A-Z0-9-]{2,}\\.?)|' #domain...\n",
    "        r'localhost|' #localhost...\n",
    "        r'\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3})' # ...or ip\n",
    "        r'(?::\\d+)?' # optional port\n",
    "        r'(?:/?|[/?]\\S+)$', re.IGNORECASE)\n",
    "    return re.match(regex, str(url)) is not None\n",
    "\n",
    "#== SCRAPER\n",
    "def get(url) -> (list, list):\n",
    "    all_p, all_a = [], []\n",
    "    response = urllib.request.urlopen(url)\n",
    "    raw_html = response.read()\n",
    "    soup = BeautifulSoup(raw_html)\n",
    "    # print(f\"\\033[92m{url}\\033[0m\\n\")\n",
    "    for p in tqdm(soup.find_all('p')):\n",
    "        p_cur = p.get_text().strip()\n",
    "        if p_cur == \"\": continue\n",
    "        all_p.append(p_cur)\n",
    "        # print(\"==>\", p_cur, \"\\n\")\n",
    "    for a in soup.find_all('a'):\n",
    "        href = a.get('href')\n",
    "        if is_url(href):\n",
    "            # print(a.get('href'))\n",
    "            all_a.append(href)\n",
    "    return all_p, all_a\n",
    "    \n",
    "#== CRAWLER \n",
    "all_p, all_a = get(URL)\n",
    "for url in all_a[0:5]:\n",
    "    _all_p, _ = get(url)\n",
    "    all_p += _all_p\n",
    "\n",
    "#== RESULT\n",
    "# FIXME: zgodnie ze specyfikacja \"cale tagi <p>\",\n",
    "#        wiec nie wydzielam pojedynczych zdan\n",
    "print(all_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zadanie 2 - CONLL\n",
    "Dane ustrukturyzowane w formacie CONLL.\n",
    "\n",
    "Niektóre algorytmy korzystają z dodatkowych metadanych opisujących poszczególne tokeny (słowa). Bardzo popularnym formatem zapisu takich danych jest format CONLL. \n",
    "\n",
    "Reprezentacja CONLL polega na tym, że dany tekst dzielony jest na zdania, a następnie każde zdanie dzielone jest na tokeny (tokenizowane). Następnie dla każdego tokenu tworzymy listę opisującą cechy tego tokenu (słowa).\n",
    "Poniżej przykład wektora opisującego każdy token zadanego tekstu:\n",
    "<ol>\n",
    "    <li>ID - numer porządkowy tokenu w zdaniu</li>\n",
    "    <li>text - tekst tokenu w formie nieprzetworzonej</li>\n",
    "    <li>Part of Speech tag (POS tag) - informacja o części mowy, która powiązana jest z tym słowem </li>\n",
    "    <li>is digit - flaga (o wartościach 0 lub 1), która informuje nas czy dany token jest liczbą</li>\n",
    "    <li>is punct - flaga (o wartościach 0 lub 1), która informuje nas czy dany token jest znakiem interpunkcyjnym</li>\n",
    "</ol>\n",
    "\n",
    "Wektory cech dla kolejnych słów zapisywane są pod sobą. **Separatorem cech w wektorze jest pojedyncza spacja.**\n",
    "\n",
    "**Zdania zwyczajowo oddzielamy od siebie podwójnym znakiem nowej linii.**\n",
    "\n",
    "Historycznie CONLL był bardzo konkretnym formatem danych w którym mieliśmy z góry narzucone cechy (np. format CONLL-U https://universaldependencies.org/docs/format.html). Liczba cech ewoluowała jednak w czasie i w wielu miejscach CONLL stał się synonimem ogólnego formatu, w którym dobór cech zależy tylko od nas, jednak stałym jest zapis sekwencji tokenów jako sekwencji wierszy w tekście, gdzie każdy wiersz jest listą oddzielonych spacją wartości (cech), a zdania oddzielone są od siebie podwójnym znakiem nowej linii.\n",
    "\n",
    "\n",
    "### Przykład:\n",
    "\n",
    "Tekst: Kasia kupiła 2 lizaki: truskawkowy i zielony. Kasia używa Apple IPhone 5 i IPad.\n",
    "\n",
    "Reprezentacja CONLL **(spacje separujące kolumny zostały zwielokrotnione na potrzeby zwiększenia czytelności)**\n",
    "<pre>\n",
    "1 Kasia  RZECZOWNIK 0 0\n",
    "2 kupiła CZASOWNIK  0 0\n",
    "3 2      LICZEBNIK  1 0\n",
    "4 lizaki RZECZOWNIK 0 0\n",
    "5 .      _          0 1\n",
    "\n",
    "1 Kasia  RZECZOWNIK 0 0\n",
    "2 używa  CZASOWNIK  0 0\n",
    "3 Apple  RZECZOWNIK 0 0\n",
    "4 IPhone RZECZOWNIK 0 0\n",
    "5 5      LICZEBNIK  1 0\n",
    "6 i      SPÓJNIK    0 0\n",
    "7 iPad   RZECZOWNIK 0 0\n",
    "8 .      _          0 1\n",
    "</pre>\n",
    "\n",
    "**Zadanie**: Napisz funkcję, która z zadanego tekstu w formie surowego tekstu stworzy reprezentację CONLL opisaną wcześniej wymienionymi atrybutami (ID, text, POS-tag, is_digit, is_punct).\n",
    "\n",
    "Wykorzystaj sentence splitter i tokenizator z NLTK. Do uzyskania informacji o POS-tagach każdego tokenu wykorzystaj funkcję nltk.pos_tag(). W kolumnie związanej z POS-tagiem zapisz pos tag w takiej formie, w jakiej uzyskamy go z funkcji pos_tag (pos_tag() zwraca formy skrótowe, np. 'NN' dla rzeczowników), nie trzeba więc zamieniać napisu \"NN\" na \"RZECZOWNIK\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1 Kate NNP 0 0\n",
      "2 uses VBZ 0 0\n",
      "3 IPhone NNP 0 0\n",
      "4 55 CD 1 0\n",
      "5 and CC 0 0\n",
      "6 IPad NNP 0 0\n",
      "7 . . 0 1\n",
      "\n",
      "1 Kate NNP 0 0\n",
      "2 bought VBD 0 0\n",
      "3 2 CD 1 0\n",
      "4 lolipops NNS 0 0\n",
      "5 . . 0 1\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "from nltk import sent_tokenize, pos_tag\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk import text_type\n",
    "tknzr = TweetTokenizer() # dobry dla nowych domen\n",
    "\n",
    "# FIXME: nigdzie nie pisze ze mam to zwrocic?\n",
    "#        wiec rozumiem ze ktos chce stdout > plik.conll\n",
    "def generate_conll(text) -> None:\n",
    "    for sent in sent_tokenize(text):\n",
    "        print(\"\")\n",
    "        tokens = tknzr.tokenize(sent)\n",
    "        pos_tokens = pos_tag(tokens)\n",
    "        i = 1\n",
    "        for pos_token in pos_tokens:\n",
    "            b1 = int(text_type.isdigit(pos_token[0]))\n",
    "            b2 = int(pos_token[0] in string.punctuation)\n",
    "            print(i, pos_token[0], pos_token[1], b1, b2)\n",
    "            i += 1\n",
    "\n",
    "\n",
    "generate_conll(\"Kate uses IPhone 55 and IPad. Kate bought 2 lolipops.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "Wyobraźmy sobie teraz, że chcielibyśmy wykrywać wzmianki o urządzeniach elektronicznych w tekście. W jaki sposób zakodować informację o (potencjalnie wielotokenowych) nazwach produktów w CONLL, tak, aby później móc wykonać proces uczenia?\n",
    "\n",
    "Dodajmy w naszym CONLLu dodatkową kolumnę reprezentującą informację o urządzeniach elektronicznych.\n",
    "Nazwy urządzeń mogą składać się potencjalnie z wielu słów.\n",
    "Do zakodowania wielotokenowych tekstów używa się najczęściej notacji IOB, gdzie każda literka skrótu oznacza interpretację aktualnego słowa:\n",
    "<ul>\n",
    "    <li> B = begin, marker, który mówi, że aktualne słowo to początek nazwy </li>\n",
    "    <li> I = inside, marker, który mówi, że aktualne słowo to kontynacja nazwy, która rozpoczyna się wystąpieniem wcześniejszego B</li>\n",
    "    <li> O = outside, marker, który mówi, że aktualne słowo nie jest interesującą nas nazwą (jest poza nią) </li>\n",
    "</ul>\n",
    "\n",
    "Po dodaniu nowej kolumny (na końcu) nasz CONLL przybiera postać:\n",
    "\n",
    "<pre>\n",
    "1 Kasia  RZECZOWNIK 0 0 O\n",
    "2 kupiła CZASOWNIK  0 0 O\n",
    "3 2                 1 0 O\n",
    "4 lizaki RZECZOWNIK 0 0 O\n",
    "5 .      _          0 1 O\n",
    "\n",
    "1 Kasia  RZECZOWNIK 0 0 O\n",
    "2 używa             0 0 O\n",
    "3 Apple  RZECZOWNIK 0 0 B\n",
    "4 IPhone RZECZOWNIK 0 0 I\n",
    "5 5                 1 0 I\n",
    "6 i      SPÓJNIK    0 0 O\n",
    "7 iPad   RZECZOWNIK 0 0 B\n",
    "8 .      _          0 1 0\n",
    "</pre>\n",
    "\n",
    "Zwróćcie Państwo uwagę na ostatnią kolumnę, czytając tekst od góry w dół, wystąpienie literki \"B\" oznacza początek interesującej frazy (Apple), jeśli zaraz za \"B\" pojawia się sekwencja oznaczona jako \"I\" - kolejne tokeny stanowią kontynuację interesującej nas frazy, w tym przypadku 3 tokeny \"Apple IPhone 5\" tworzą jeden byt. Poza tym widzimy, że \"iPad\" stanowi osobny, jednotokenowy byt.\n",
    "\n",
    "Po co rozróżniać pomiędzy \"B\", \"I\" i \"O\", czy nie można uwzględnić tylko dwóch tagów \"wewnątrz frazy\", \"poza frazą\"? Teoretycznie można, ale wprowadzimy w ten sposób sytuacje niejednoznaczne. \n",
    "\n",
    "Sprawdźmy to na przykładzie sekwencji \"XBox Playstation\" reprezentującej 2 osobne byty. Używając tagowania IOB nasza sekwencja wyglądałaby tak:\n",
    "\n",
    "XBox B\n",
    "PlayStation B\n",
    "\n",
    "Widzimy więc, że dwa tagi \"B\" oznaczają dwa początki osobnych fraz. Co jednak gdybyśmy używali tagów \"wewnątrz (interesującej nas) frazy\", \"poza (interesującą nas) frazą\"?\n",
    "\n",
    "XBox \"wewnątrz (interesującej nas) frazy\"\n",
    "Playstation \"wewnątrz (interesującej nas) frazy\"\n",
    "\n",
    "W tej sytuacji oznaczyliśmy poprawnie oba tokeny jako części interesujących nas fraz. Jednak nie wiemy, czy XBox Playstation to jedna, czy dwie osobne frazy (byty) -- stąd format IOB jest zdecydowanie bezpieczniejszym wyborem.\n",
    "\n",
    "**Zadanie**: Napisz funkcję, która wygeneruje CONLL z uwzględnieniem tagów IOB dotyczących urządzeń.\n",
    "Nasza funkcja posiada teraz dodatkowy argument devices, który zawiera listę obiektów, które opisują gdzie (przesunięcie znakowe) znajduje się początek i koniec wzmianek.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DEBUG ===\n",
      "[{'begin': 10, 'end': 18}, {'begin': 23, 'end': 27}]\n",
      "@1:  Kate uses IPhone 5 and IPad. Kate bought 2 lolipops.\n",
      "@2:  Kate uses XXXXXX X and XXXX. Kate bought 2 lolipops.\n",
      "=== DEBUG ===\n",
      "\n",
      "1 Kate NNP 0 0 O\n",
      "2 uses VBZ 0 0 O\n",
      "3 IPhone NNP 0 0 B\n",
      "4 5 CD 1 0 I\n",
      "5 and CC 0 0 O\n",
      "6 IPad NNP 0 0 B\n",
      "7 . . 0 1 O\n",
      "\n",
      "1 Kate NNP 0 0 O\n",
      "2 bought VBD 0 0 O\n",
      "3 2 CD 1 0 O\n",
      "4 lolipops NNS 0 0 O\n",
      "5 . . 0 1 O\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "from nltk import sent_tokenize, pos_tag\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk import text_type\n",
    "from pprint import pprint\n",
    "from copy import copy\n",
    "tknzr = TweetTokenizer()\n",
    "\n",
    "def generate_CONLL(text, devices=[]) -> None:\n",
    "    # zrobione jest tak aby w wyniku tokenizacji,\n",
    "    # nie przesunely sie wyrazy miedzy soba i nie zaznaczyc\n",
    "    # przez to innych\n",
    "    print(\"=== DEBUG ===\")\n",
    "    pprint(devices)\n",
    "    text_mask = copy(text)\n",
    "    for device in devices:\n",
    "        masked = text_mask[device[\"begin\"]:device[\"end\"]]\\\n",
    "                 .translate({ord(c):'X' for c in string.printable.replace(\" \", \"\")})\n",
    "        text_mask = text_mask[0:device[\"begin\"]] + masked + text_mask[device[\"end\"]:]\n",
    "    print(\"@1: \", text)\n",
    "    print(\"@2: \", text_mask)\n",
    "    print(\"=== DEBUG ===\")\n",
    "    sents = zip(sent_tokenize(text), sent_tokenize(text_mask))\n",
    "    for sent, sent_mask in sents:\n",
    "        print(\"\")\n",
    "        tokens = tknzr.tokenize(sent)\n",
    "        tokens_mask = tknzr.tokenize(sent_mask)\n",
    "        pos_tokens = pos_tag(tokens)\n",
    "        pos_tokens_mask = pos_tag(tokens_mask)\n",
    "        i = 1\n",
    "        last_unicorn = False\n",
    "        for pos_token, pos_token_mask in zip(pos_tokens, pos_tokens_mask):\n",
    "            b1 = int(text_type.isdigit(pos_token[0]))\n",
    "            b2 = int(pos_token[0] in string.punctuation)\n",
    "            _unicorn_char = list(set(pos_token_mask[0]))\n",
    "            is_unicorn = _unicorn_char[0] == \"X\" and len(_unicorn_char) == 1\n",
    "            if is_unicorn and not last_unicorn:\n",
    "                b3 = \"B\"\n",
    "            elif is_unicorn and last_unicorn:\n",
    "                b3 = \"I\"\n",
    "            else:\n",
    "                b3 = \"O\"\n",
    "            print(i, pos_token[0], pos_token[1], b1, b2, b3)\n",
    "            last_unicorn = is_unicorn\n",
    "            i += 1\n",
    "\n",
    "# parametr devices to lista słowników w którym mamy informację o numerze znaku na którym fraza się zaczyna i kończy (zobacz: próba wywołania w ostatniej linijce) (litera I z Iphone występuje na 10 znaku)\n",
    "# Zapoznaj się z dokumentacją SpaCy (obiekt Token), aby zobaczyć jak wydobyć informację o pozycji danego słowa w zdaniu/dokumencie.\n",
    "    \n",
    "generate_CONLL(\"Kate uses IPhone 5 and IPad. Kate bought 2 lolipops.\", devices=[{\"begin\": 10, \"end\":18}, {\"begin\": 23, \"end\": 27}])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Często chcemy w tekście naraz oznaczać byty, które należą do różnych kategorii, np. lokacje, numery telefonów, daty, wzmianki o osobach. W takich sytuacjach używa się również kodowania IOB jednak wzbogaca się etykiety o odpowiednie informacje używając formatu:\n",
    "\n",
    "{tag IOB}-{etykieta kategorii}\n",
    "\n",
    "Stąd daty przyjmują oznaczenia: B-DATE / I-DATE, osoby B-PERSON / I-PERSON, numery telefonów B-PHONENUMBER / I-PHONENUMBER, lokacje: B-LOCATION / I-LOCATION itp. Wiemy zatem czy dany token należy do interesującej nas frazy i do jakiej kategorii przypisana jest ta fraza."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
